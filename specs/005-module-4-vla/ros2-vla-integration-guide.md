# ROS 2 VLA Workflow Integration Guide

**Document Purpose**: Technical reference for integrating Navigation2, Isaac ROS, and MoveIt2 into a cohesive Voice-Language-Action pipeline for humanoid robots

**Date**: 2025-12-21
**Target Audience**: Module 4 content creators, roboticists implementing VLA systems

---

## Executive Summary

This guide documents how **three official ROS 2 frameworks** combine to create a complete VLA system:

1. **Navigation2 (Nav2)** — High-level navigation and path planning via action servers
2. **NVIDIA Isaac ROS** — Real-time GPU-accelerated perception and SLAM
3. **MoveIt2** — Manipulation planning and trajectory execution for robotic arms

**Key Innovation**: All three frameworks communicate via **standardized ROS 2 action servers and topics**, enabling LLM-generated task plans to be transparently executed on any robot.

---

## VLA System Architecture

### The Complete Pipeline

```
VOICE INPUT
    ↓
Speech Recognition (OpenAI Whisper)
    ↓
Natural Language: "Pick up the red mug from the table and bring it to the kitchen"
    ↓
LLM Task Decomposition (GPT-4, Claude, PaLM)
    ├─ Subtask 1: Navigate to table location
    ├─ Subtask 2: Detect red mug via vision
    ├─ Subtask 3: Plan and execute grasp
    ├─ Subtask 4: Navigate to kitchen
    └─ Subtask 5: Release mug
    ↓
ROS 2 Action Server Invocation (in execution order)
    ├─ Nav2: NavigateToPose action (table location)
    ├─ Isaac ROS: Object detection topic (find mug)
    ├─ MoveIt2: MoveGroup action (arm to grasp pose)
    ├─ Nav2: NavigateToPose action (kitchen)
    └─ MoveIt2: Open gripper action (release)
    ↓
Real-Time Vision Feedback Loop
    ├─ Isaac ROS Visual SLAM: /visual_slam/tracking/odometry (localization)
    ├─ Isaac ROS DNN Inference: /detection_results (object tracking)
    └─ Depth Sensor: /camera/depth/points (obstacle avoidance)
    ↓
Execution Monitoring & Replanning
    ├─ Monitor action status (success/failure/timeout)
    ├─ Check vision for plan invalidation (e.g., mug moved)
    └─ Trigger replanning if needed
    ↓
SUCCESS: Mug delivered to kitchen
FAILURE: LLM replans with new strategy
```

---

## Framework Integration Details

### 1. Navigation2 (Nav2) for Movement

**Role in VLA**: Executes high-level navigation commands generated by LLM

#### Action Server Interface

```yaml
# ROS 2 Action Definition
action: nav2_msgs/NavigateToPose

# LLM generates: "Navigate to kitchen (map coordinates [3.5, 2.1, 0])"
goal:
  pose:
    header:
      frame_id: "map"
    pose:
      position: {x: 3.5, y: 2.1, z: 0}
      orientation: {x: 0, y: 0, z: 0.707, w: 0.707}  # 90° rotation
  behavior_tree_xml: ""  # Uses default behavior tree

# Nav2 provides feedback during execution
feedback:
  current_pose:
    header:
      frame_id: "map"
    pose: {x: 1.2, y: 1.8, z: 0}  # Real-time robot position
  number_of_recoveries: 0
  distance_remaining: 2.4

# Final result
result:
  error_code: 0  # 0 = success, non-zero = failure type
```

#### Topics Published by Nav2

| Topic | Message Type | Purpose |
|---|---|---|
| `/plan` | nav_msgs/Path | Global path waypoints |
| `/cmd_vel` | geometry_msgs/Twist | Velocity commands to base controller |
| `/amcl_pose` | geometry_msgs/PoseWithCovarianceStamped | Localization (fused from Isaac ROS VSLAM + IMU) |
| `/costmap/costmap` | nav_msgs/OccupancyGrid | Obstacle map (-1=unknown, 0=free, 100=obstacle) |
| `/local_costmap/obstacles` | sensor_msgs/PointCloud2 | Dynamic obstacles from perception |

#### VLA Integration Code Pattern

```python
import rclpy
from nav2_msgs.action import NavigateToPose
from rclcpp_action import CancelResponse

class VLANavigationClient:
    def __init__(self, node):
        self.client = node.create_action_client(NavigateToPose, 'navigate_to_pose')

    async def navigate_to(self, x: float, y: float, theta: float) -> bool:
        """LLM calls: navigate_to(3.5, 2.1, 0)"""
        goal = NavigateToPose.Goal()
        goal.pose.header.frame_id = 'map'
        goal.pose.pose.position.x = x
        goal.pose.pose.position.y = y

        # Convert theta to quaternion
        goal.pose.pose.orientation.z = math.sin(theta / 2)
        goal.pose.pose.orientation.w = math.cos(theta / 2)

        # Send goal and wait for result
        send_goal_future = self.client.send_goal_async(goal)
        goal_handle = await send_goal_future

        if not goal_handle.accepted:
            return False

        result_future = goal_handle.get_result_async()
        result = await result_future

        return result.result.error_code == 0  # 0 = success
```

**Humanoid Consideration**: Nav2's `/cmd_vel` output (2D velocity) requires humanoid-specific interpretation layer to convert to balance control + joint commands. See MoveIt2 section for whole-body planning alternative.

---

### 2. Isaac ROS for Real-Time Perception

**Role in VLA**: Provides vision feedback (localization, object detection, depth) that informs action refinement

#### Core Packages for VLA

**A. isaac_ros_visual_slam** — Real-time localization

```yaml
Subscribed Topics:
  /left/image_raw: sensor_msgs/Image  # Left stereo camera
  /right/image_raw: sensor_msgs/Image  # Right stereo camera
  /left/camera_info: sensor_msgs/CameraInfo  # Calibration
  /right/camera_info: sensor_msgs/CameraInfo

Published Topics:
  /visual_slam/tracking/odometry: nav_msgs/Odometry
    # Header with frame_id: "odom"
    # Pose: {x, y, z, roll, pitch, yaw}
    # Velocity: {linear, angular}
    # Covariance: position uncertainty (for sensor fusion)

  /visual_slam/vis/slam_path: nav_msgs/Path
    # Robot's complete trajectory (useful for debugging)

  /visual_slam/vis/landmarks_cloud: sensor_msgs/PointCloud2
    # 3D feature map for visualization

Performance:
  Rate: 60 FPS at 1280×720 resolution
  Latency: ~16 ms from capture to odometry output
```

**VLA Integration**: Nav2 uses this odometry to continuously update robot pose. Humanoid balance controller uses angular orientation (roll, pitch) for IMU feedback.

**B. isaac_ros_dnn_image_encoder** — Object detection for vision-guided action

```yaml
Subscribed Topics:
  /camera/image: sensor_msgs/Image  # RGB image

Published Topics:
  /detection_results: isaac_ros_pose_estimation_3d_msgs/DetectionResult3DArray
    detections[]:
      results[]:
        hypothesis:
          class_name: "mug"  # From training
          probability: 0.94
      bbox:
        center: {x: 640, y: 360}  # Pixel coordinates
        size_x: 80
        size_y: 100
      pose:  # 6D pose from pose estimation
        position: {x: 0.5, y: -0.2, z: 1.2}  # Meters from camera
        orientation: {x, y, z, w}

Performance:
  Rate: 30 FPS for detection, 120 FPS with INT8 quantization
  Model: YOLO-v5 or RT-1/RT-2 (VLA-specific)
```

**VLA Integration**: When LLM command is "grasp the red mug":
1. Filter detections: `class == "mug" AND color == "red"`
2. If multiple detections: pick closest (use depth)
3. Pass 3D pose to MoveIt2 for grasp planning

**C. isaac_ros_stereo_image_proc** — Dense depth estimation

```yaml
Subscribed Topics:
  /left/image_raw: sensor_msgs/Image
  /right/image_raw: sensor_msgs/Image

Published Topics:
  /camera/depth/image_rect: sensor_msgs/Image
    # Depth map: each pixel value = distance in mm
    # Can be converted to PointCloud2 for 3D obstacle avoidance

  /camera/depth/points: sensor_msgs/PointCloud2
    # 3D points for Nav2 costmap layer
    fields: [x, y, z, rgb]

Performance:
  Rate: 60 FPS at 1280×720 (Semi-Global Matching algorithm)
```

**VLA Integration**: Feeds into Nav2's dynamic costmap layer for real-time obstacle avoidance.

#### Sensor Fusion with Navigation

```python
# Nav2 typically fuses multiple sources:
robot_localization_ekf:
  inputs:
    - isaac_ros_visual_slam/odometry  # Primary: visual odometry
    - imu_data  # Gyroscope for orientation correction
    - wheel_odometry  # Dead reckoning backup (for humanoid, less relevant)

  output:
    /odometry/filtered  # Published at 50 Hz

# Nav2 AMCL then uses /odometry/filtered with map for localization
nav2_amcl:
  input_odometry: /odometry/filtered
  input_lidar: /camera/depth/points  # From Isaac ROS stereo
  output: /amcl_pose  # Used by planner for replanning
```

---

### 3. MoveIt2 for Robotic Manipulation

**Role in VLA**: Plans and executes arm motion to achieve grasping and manipulation tasks

#### Action Server Interface

```yaml
# ROS 2 Action Definition
action: moveit_msgs/MoveGroup

# LLM generates: "Grasp red mug at [0.5, -0.2, 1.2]"
goal:
  request:
    group_name: "manipulator"  # Arm joint group (usually 7-DOF or humanoid arm)
    num_planning_attempts: 10
    allowed_planning_time: 5.0
    max_velocity_scaling_factor: 0.5  # Safety: reduce speed
    max_acceleration_scaling_factor: 0.5

    goal_constraints[0]:
      position_constraints[0]:
        link_name: "end_effector_link"
        target_point: {x: 0.5, y: -0.2, z: 1.2}
        constraint_region:
          primitive_shapes[0]:
            type: 1  # SPHERE shape
            dimensions: [0.02]  # 2cm radius tolerance

      orientation_constraints[0]:
        link_name: "end_effector_link"
        orientation: {x, y, z, w}  # Desired gripper orientation
        absolute_x_axis_tolerance: 0.1  # Radians
        absolute_y_axis_tolerance: 0.1
        absolute_z_axis_tolerance: 0.1

    workspace_parameters:
      header:
        frame_id: "base_link"
      min_corner: {x: -1.0, y: -1.5, z: 0.0}
      max_corner: {x: 1.5, y: 1.5, z: 2.5}

feedback:
  state: "PLANNING"

result:
  error_code: 0  # 0 = success, see MoveItErrorCodes for others
  trajectory_response:
    trajectory:
      joint_trajectory:
        joint_names: ["shoulder_pan", "shoulder_lift", "elbow_flex", ...]
        points[]:
          positions: [0.5, 1.2, -0.8, ...]  # Joint angles in radians
          velocities: [0.1, 0.2, 0.0, ...]
          time_from_start: {secs: 2, nsecs: 500000}  # 2.5 seconds
```

#### Topics for Manipulation Feedback

| Topic | Message Type | Purpose |
|---|---|---|
| `/joint_states` | sensor_msgs/JointState | Real-time arm configuration (used for IK solving) |
| `/tf/tf` | tf2_msgs/TFMessage | Frame transforms (end_effector relative to base_link) |
| `/move_group/display_planned_path` | moveit_msgs/DisplayTrajectory | Visualization of planned trajectory |
| `/move_group/plan_execution/action_goal` | moveit_msgs/ExecuteTrajectoryGoal | Trajectory being executed |

#### VLA Integration: Vision-Guided Grasping

```python
class VLAManipulationClient:
    def __init__(self, node):
        self.move_group_client = node.create_action_client(
            MoveGroup, '/move_group'
        )
        self.execute_client = node.create_action_client(
            ExecuteTrajectory, '/execute_trajectory'
        )

    async def grasp_object(self, obj_pose_3d, obj_class: str):
        """
        LLM provides:
        - obj_pose_3d: [x, y, z, roll, pitch, yaw] from Isaac ROS detection
        - obj_class: "mug", "cup", "bottle", etc.
        """

        # Step 1: Compute grasp pose from object pose
        grasp_pose = self.compute_grasp_pose(obj_pose_3d, obj_class)

        # Step 2: Plan arm trajectory to grasp
        plan_goal = MoveGroup.Goal()
        plan_goal.request.group_name = "manipulator"
        # Set goal_constraints to grasp_pose (as above)

        goal_handle = await self.move_group_client.send_goal_async(plan_goal)
        result = await goal_handle.get_result_async()

        if result.result.error_code != 0:
            print(f"Planning failed: {result.result.error_code}")
            return False

        planned_trajectory = result.result.trajectory_response.trajectory

        # Step 3: Execute trajectory with real-time feedback
        exec_goal = ExecuteTrajectory.Goal()
        exec_goal.trajectory = planned_trajectory

        exec_handle = await self.execute_client.send_goal_async(exec_goal)
        exec_result = await exec_handle.get_result_async()

        if exec_result.result.error_code != 0:
            print("Execution failed")
            return False

        # Step 4: Close gripper
        await self.close_gripper(force=50.0)  # Newtons

        return True
```

#### Humanoid Whole-Body Coordination

For humanoid robots, manipulation is **not independent of locomotion**:

```python
# Simplified example: Humanoid reaching for object on high shelf

# Step 1: Walk to position (Nav2)
await nav_client.navigate_to(x=1.0, y=0.5, theta=0)

# Step 2: Detect object height
obj_height = detected_mug.pose.position.z  # 1.8 meters (high shelf)

# Step 3: MoveIt2 whole-body planning
# Considers:
# - Arm stretch (7 DOF + end-effector)
# - Torso lift (1 DOF)
# - Whole-body balance constraints
# - Center of mass must remain above feet

whole_body_goal = MoveGroup.Goal()
whole_body_goal.request.group_name = "whole_body"  # All active DOF
whole_body_goal.request.goal_constraints[0] = <grasp_pose_at_shelf_height>

# MoveIt2 plans collision-free, balance-safe trajectory
# Result: Walk closer + lift torso + raise arm to grasp
```

---

## ROS 2 Message Type Reference for VLA

### Perception → Planning Interface

```cpp
// Isaac ROS → MoveIt2: Object detected, need to grasp

// Detection output (Isaac ROS)
isaac_ros_pose_estimation_3d_msgs::DetectionResult3DArray detection;
// Contains:
//   detection.detections[0].pose.position  → 3D location
//   detection.detections[0].pose.orientation  → Grasping axis

// Convert to MoveIt grasp goal
geometry_msgs::PoseStamped grasp_target;
grasp_target.header.frame_id = "base_link";
grasp_target.pose.position = detection.detections[0].pose.position;
grasp_target.pose.orientation = detection.detections[0].pose.orientation;

// Pass to MoveIt2
moveit_msgs::MoveGroupGoal move_goal;
move_goal.request.goal_constraints[0].position_constraints[0]
    .target_point = grasp_target.pose.position;
```

### Navigation → Action Interface

```cpp
// LLM task: "Navigate to kitchen"
// Kitchen location from semantic map: x=3.5, y=2.1

nav2_msgs::NavigateToPose::Goal nav_goal;
nav_goal.pose.header.frame_id = "map";
nav_goal.pose.pose.position.x = 3.5;
nav_goal.pose.pose.position.y = 2.1;

// Monitor progress via /amcl_pose topic
geometry_msgs::PoseWithCovarianceStamped current_pose;
// If robot stalled: check /move_base_flex/feedback for stuck detection
// Trigger replanning: "Try alternate route"
```

### Arm → Hand Coordination

```cpp
// After arm reaches grasp pose:
// Publish gripper command

control_msgs::GripperCommand gripper_cmd;
gripper_cmd.position = 0.0;  // Fully closed (0-1 range: 0=closed, 1=open)
gripper_cmd.effort = 50.0;   // Maximum force in Newtons

// Monitor gripper feedback
sensor_msgs::JointState gripper_state;
// gripper_state.position[0] = actual position (0.0-0.05m)
// If object slips (force drops): trigger regrasp
```

---

## VLA Execution Flow with Error Handling

### Task: "Pick up the blue cup from the kitchen table"

```
┌──────────────────────────────────────────────────────────┐
│ LLM Task Decomposition                                   │
├──────────────────────────────────────────────────────────┤
│ Subtask 1: Navigate to kitchen (Nav2)                    │
│ Subtask 2: Detect blue cup (Isaac ROS)                   │
│ Subtask 3: Plan arm trajectory (MoveIt2)                 │
│ Subtask 4: Execute grasping (MoveIt2 + Gripper)         │
└──────────────────────────────────────────────────────────┘
                    ↓ Sequential Execution
┌──────────────────────────────────────────────────────────┐
│ SUBTASK 1: Navigate to Kitchen                           │
├──────────────────────────────────────────────────────────┤
│ Nav2 Action: NavigateToPose(3.5, 2.1, 0)               │
│                                                          │
│ ✅ Success Path:                                         │
│   Route planned → Follow waypoints → Arrive at goal     │
│   Duration: ~5 seconds                                   │
│   Transition: Proceed to Subtask 2                      │
│                                                          │
│ ❌ Failure Path 1: Obstacle blocks path                 │
│   Nav2 replans (local costmap updated by stereo depth)  │
│   If replan succeeds → resume navigation                │
│   If replan fails: LLM triggered with "PATH_BLOCKED"    │
│   → LLM generates alternate route                       │
│                                                          │
│ ❌ Failure Path 2: Localization lost                     │
│   Isaac ROS VSLAM lost tracking (dark area, fast motion)│
│   Recovery behavior: Rotate in place to regain visual   │
│   features, then continue                               │
└──────────────────────────────────────────────────────────┘
                    ↓ (Success assumed)
┌──────────────────────────────────────────────────────────┐
│ SUBTASK 2: Detect Blue Cup                               │
├──────────────────────────────────────────────────────────┤
│ Isaac ROS DNN: Query /detection_results                  │
│                                                          │
│ ✅ Success Path:                                         │
│   "blue" + "cup" detected at [0.8, -0.3, 0.9] meters   │
│   Confidence: 0.92                                       │
│   Proceed to Subtask 3                                   │
│                                                          │
│ ❌ Failure Path: Object not found                        │
│   Retry with object localization: Query LLM             │
│   "Cup not visible. Check under table or behind other" │
│   items."                                                │
│   OR: Human prompts: "Move closer to table"             │
│   → Nav2: NavigateToPose closer to table (Subtask 1')   │
│   → Retry detection                                      │
└──────────────────────────────────────────────────────────┘
                    ↓ (Success assumed)
┌──────────────────────────────────────────────────────────┐
│ SUBTASK 3: Plan Arm Trajectory (Grasp)                   │
├──────────────────────────────────────────────────────────┤
│ MoveIt2 Action: MoveGroup                                │
│   goal_constraints: position=[0.8, -0.3, 0.9]           │
│                    orientation=[roll, pitch, yaw]       │
│   allowed_planning_time: 5.0 seconds                     │
│                                                          │
│ ✅ Success Path:                                         │
│   Collision-free trajectory computed (10 waypoints)     │
│   Trajectory duration: 2.3 seconds                       │
│   Proceed to Subtask 4                                   │
│                                                          │
│ ❌ Failure Path: IK infeasible                           │
│   Cup location unreachable from current robot base      │
│   Error: "Goal unreachable with current arm config"     │
│   LLM replans: "Move robot base closer" (invoke Nav2)   │
│   Trigger Subtask 1' with new navigation goal           │
│                                                          │
│ ❌ Failure Path: Collision detected                       │
│   Planned trajectory would hit table edge                │
│   MoveIt2 reports: "Collision at waypoint 5"            │
│   LLM adjusts: "Approach from side instead of front"    │
│   MoveIt2 replans with different cost-weighted approach │
└──────────────────────────────────────────────────────────┘
                    ↓ (Success assumed)
┌──────────────────────────────────────────────────────────┐
│ SUBTASK 4: Execute Grasping                              │
├──────────────────────────────────────────────────────────┤
│ Step 4a: Execute arm trajectory                          │
│   control_msgs/FollowJointTrajectory action             │
│   Real-time feedback: Current joint angles, progress     │
│                                                          │
│ ✅ Success: Arm reaches grasp pose (within 0.05m)       │
│                                                          │
│ ❌ Failure: Joint torque limit exceeded                  │
│   (Object moved, or collision detected mid-trajectory)   │
│   Recovery: Halt → Recompute IK → Replan                │
│                                                          │
│ Step 4b: Close gripper                                   │
│   GripperCommand: position=0.0 (closed), effort=50N     │
│                                                          │
│ ✅ Success: Gripper force > 10N (object grasped)        │
│   Feedback: gripper_state.position = 0.02 (partially   │
│   closed, indicating object contact)                     │
│                                                          │
│ ❌ Failure: Object slips (force drops to 0)              │
│   Recovery: Increase grip force → Retry grasp            │
│                                                          │
│ Final Result: Cup successfully grasped                   │
│ Transition: Task complete or continue with delivery      │
└──────────────────────────────────────────────────────────┘
```

### Replanning Trigger Logic

```python
def monitor_vla_execution(subtask_result):
    """Determine if replanning needed"""

    if subtask_result.error_code == 0:
        return "CONTINUE"  # Success, move to next subtask

    elif subtask_result.error_code == "PATH_BLOCKED":
        # Obstacle detected, Nav2 cannot replan
        llm_input = "Path to kitchen is blocked. Suggest alternative route."
        new_plan = llm.replan(llm_input)
        return "REPLAN"

    elif subtask_result.error_code == "LOCALIZATION_LOST":
        # VSLAM tracking lost
        recovery = "Rotate in place to restore visual tracking"
        return "RECOVERY"

    elif subtask_result.error_code == "IK_INFEASIBLE":
        # Goal unreachable
        llm_input = "Cup at [0.8, -0.3, 0.9] is unreachable. Move robot 0.3m closer."
        new_plan = llm.replan(llm_input)
        return "REPLAN"

    elif subtask_result.error_code == "GRIPPER_SLIP":
        # Object dropped
        llm_input = "Cup slipped from gripper. Retry grasp with higher force."
        new_plan = llm.replan(llm_input)
        return "REPLAN"
```

---

## Implementation Checklist for Module 4 Content

When writing VLA chapters, ensure these topics are covered:

### Chapter 1: VLA Fundamentals
- [ ] Define Vision, Language, Action as distinct modalities
- [ ] Show how they interconnect (voice → understanding → physical action)
- [ ] Reference ROS 2 as the middleware enabling integration
- [ ] Explain action servers as the bridge between LLM plans and robot execution

### Chapter 2: Voice-to-Action Pipeline
- [ ] Whisper speech recognition (input: audio, output: text with confidence)
- [ ] Text preprocessing (punctuation, capitalization normalization)
- [ ] Reference to ROS 2 topics for audio I/O (sensor_msgs/Audio from microphone)

### Chapter 3: Language Planning with LLMs
- [ ] Task decomposition (break "pick up cup" into subtasks)
- [ ] Mapping to ROS 2 action primitives (Nav2 NavigateToPose, MoveIt2 MoveGroup)
- [ ] Few-shot prompting examples
- [ ] Error handling and replanning triggers

### Chapter 4: Vision-Guided Action + Capstone
- [ ] Isaac ROS perception pipeline (VSLAM for localization, DNN for detection)
- [ ] Closed-loop control: vision feedback → action adjustment
- [ ] MoveIt2 grasp planning from detected objects
- [ ] Complete capstone workflow tying all components
- [ ] Simulation-first validation in Isaac Sim before hardware

### Module Cross-References
- [ ] Module 1: ROS 2 action servers, topics, services
- [ ] Module 2: Gazebo simulation environment for testing
- [ ] Module 3: Isaac Sim training, Isaac ROS packages, Nav2 fundamentals
- [ ] Module 4: VLA integration bringing it all together

---

## Quick Reference: ROS 2 Action Invocation Patterns

### Pattern 1: Simple Navigation

```python
# LLM generates: "Go to living room"
nav_goal = NavigateToPose.Goal()
nav_goal.pose.pose.position.x = 5.0
nav_goal.pose.pose.position.y = 3.0
goal_handle = nav_client.send_goal_async(nav_goal)
result = await goal_handle.get_result_async()
success = (result.result.error_code == 0)
```

### Pattern 2: Object Detection + Grasp

```python
# LLM generates: "Grasp the red ball"
detections = await listen_to_perception_topic()
red_balls = [d for d in detections if d.class=="ball" and d.color=="red"]
target = red_balls[0].pose  # Take first/closest

move_goal = MoveGroup.Goal()
move_goal.request.goal_constraints[0].position_constraints[0].target_point = target.position
goal_handle = move_client.send_goal_async(move_goal)
trajectory = (await goal_handle.get_result_async()).trajectory
```

### Pattern 3: Multi-Step Task with Replanning

```python
# LLM task: [step1, step2, step3]
for step in task_plan:
    result = await execute_ros_action(step)
    if result.error_code != 0:
        # Failure: ask LLM to replan
        failure_reason = error_code_to_text(result.error_code)
        new_plan = llm.replan(f"Failed: {failure_reason}. What's next?")
        task_plan = new_plan  # Update plan and continue
```

---

## Key Insights for Content Writers

1. **ROS 2 is the Enabler**: All three frameworks (Nav2, Isaac ROS, MoveIt2) speak the same ROS 2 language (action servers, topics, services). VLA systems leverage this standardization.

2. **Real-Time Vision is Critical**: Isaac ROS's GPU acceleration is essential — without <50ms perception latency, closed-loop control fails and robots appear unresponsive.

3. **Humanoid Complexity**: Standard Nav2 assumes 2D wheels. Humanoids need whole-body coordination via MoveIt2 + balance control. This is a key differentiator from wheeled robots.

4. **Replanning is Constant**: Perfect LLM plans are rare. Most VLA systems spend 70% of runtime in failure recovery and replanning. This is not a limitation; it's the expected operational mode.

5. **Sim-to-Real is Non-Trivial**: Train and test everything in Isaac Sim first. Domain randomization is critical for perception robustness in real-world lighting/clutter conditions.

---

## External References

For detailed implementation:

- **Nav2 Documentation**: https://docs.nav2.org/
- **Isaac ROS Documentation**: https://docs.nvidia.com/isaac-ros/latest/
- **MoveIt2 Documentation**: https://moveit.ros.org/
- **ROS 2 Action Standard**: https://docs.ros.org/en/humble/Tutorials/Beginner-CLI-Tools/Understanding-ROS2-Actions/Understanding-ROS2-Actions.html
